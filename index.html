<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>R$\theta$VLM-TAMP: Enhancing Long-Horizon Robotic Manipulation with Reasoning Visual-Language Models</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            color: #333;
        }
        header {
            background-color: #f8f9fa;
            padding: 20px;
            text-align: center;
        }
        main {
            max-width: 1200px;
            margin: 20px auto;
            padding: 0 20px;
        }
        h1, h2 {
            color: #4CAF50;
        }
        .abstract {
            background-color: #f0f0f0;
            padding: 20px;
            border-radius: 8px;
            margin-bottom: 20px;
        }
        .figures {
            display: flex;
            gap: 20px;
            flex-wrap: wrap;
            justify-content: center;
        }
        .figure {
            max-width: 500px;
            text-align: center;
        }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
        }
        footer {
            background-color: #f8f9fa;
            padding: 10px;
            text-align: center;
            margin-top: 40px;
        }
    </style>
</head>
<body>
    <header>
        <h1>R$\theta$VLM-TAMP: Enhancing Long-Horizon Robotic Manipulation with Reasoning Visual-Language Models</h1>
    </header>
    <main>
        <section class="abstract">
            <h2>Abstract</h2>
            <p>Robots have long faced challenges in long-horizon manipulation, especially in open-goal scenarios, where robots are typically required to  interact with the environment and perform a series of actions to accomplish open-semantic tasks.Traditional approaches based  on Task and Motion Planning (TAMP) struggle with open-world semantics.</p>
            <p>In this paper, we propose R$\theta$VLM-TAMP, a framework that integrates reasoning-enhanced Vision-Language Models (Reasoning VLMs) with robotic systems for long-horizon tasks requiring both reasoning and manipulation. R$\theta$VLM-TAMP leverages the capabilities of Reasoning VLMs to accomplish long-horizon manipulation tasks. In addition, we introduce an enhancement method that improves VLMsâ€™ reasoning capabilities specifically for manipulation tasks. Through comparative experiments, our approach achieves a 14\%--19\% performance improvement over baselines across varying difficulty levels. </p>
        </section>
        <section class="figures">
            <div class="figure">
                <img src="static/images/overview_v7.png" alt="R$\theta$VLM-TAMP Approach">
                <p><strong>Figure 1:</strong> Architecture of the R$\theta$VLM-TAMP</p>
            </div>
            <div class="figure">
                <img src="static/images/exp_plot02.png" alt="Experiment Results">
                <p><strong>Figure 2:</strong> Box plot of average success rates across ten scenarios for different methods at different levels of difficulty.</p>
            </div>
        </section>
    </main>
    <footer>
        <p>&copy; 2025kvkcon. All rights reserved.</p>
    </footer>
</body>
</html>
